{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sb\n",
    "import re\n",
    "import datetime as dt\n",
    "import glob\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENSOR AGGREGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This had to be done because some of the prior csvs in these folder didn't have the same data with no real pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This had to be done because some of the prior csvs in these folder didn't have the same data with no real pattern\n",
    "directs_chest = [\"/mnt/storage/Datasets/Step Study/BioStampRC/Step 01/Step Study 01/step_01/medial_chest/d5la7xya/2018-11-08T23-49-48-420Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 02/Step Study 02/medial_chest/d5la7xya/2018-11-14T23-45-02-785Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 03/Step 03/step 03/medial_chest/d5la7xya/2018-12-06T23-13-40-076Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 04/Step Study 04/step4/medial_chest/d5la7xya/2019-01-25T23-06-45-750Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 05/Step Study 05/step05/medial_chest/d5la7xqf/2019-02-06T16-04-20-022Z\",        \n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 06/Step Study 06/step 06/medial_chest/d5la7xqf/2019-02-08T22-52-27-184Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 07/Step Study 07/step 07/medial_chest/d5la7xqf/2019-02-23T18-40-28-633Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 08/Step Study 04/step4/medial_chest/d5la7xya/2019-01-28T16-36-18-467Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 10/Step Study 10/step 10/medial_chest/d5la7xya/2019-02-21T23-24-54-381Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 11/Step Study 11/step 11/medial_chest/d5la7xqf/2019-02-23T00-54-16-601Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 12/Step Study 12/step 12/medial_chest/d5la7xya/2019-02-15T23-00-10-002Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 13/Step Study 13/step 13/medial_chest/d5la7xya/2019-03-22T17-56-46-441Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 14/Step Study 14/step 14/medial_chest/d5la7xya/2019-04-25T23-16-48-211Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 15/medial_chest/d5la7xya/2019-06-27T00-29-38-808Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 16/step 16/medial_chest/d5la7xya/2019-06-28T19-06-43-477Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 17/Step Study 17/step 17/medial_chest/d5la7xqf/2019-03-14T23-12-16-246Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 18/Step Study 18/Step18/medial_chest/d5la7xqf/2019-04-23T22-19-47-919Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 19/Step Study 19/step 19/medial_chest/d5la7xya/2019-04-30T23-14-57-400Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 20/Step Study 20/step 20/medial_chest/d5la7xya/2019-06-12T21-20-12-037Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 21/Step Study 21/medial_chest/d5la7xqf/2019-07-10T18-37-47-380Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 22/Step Study 22/step 22/medial_chest/d5la7xya/2019-04-29T21-22-08-481Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 23/Step Study 23/step 23/medial_chest/d5la7xya/2019-04-22T23-53-48Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 24/Step Study 24/step 24/medial_chest/d5la7xqf/2019-06-14T19-39-13-232Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 25/Step Study 25/step 25/medial_chest/d5la7xqf/2019-05-14T23-53-02-467Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 27/Step Study 27/step 27/medial_chest/d5la7xya/2019-05-03T22-16-03-808Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 28/Step Study 28/step study 28/medial_chest/d5la7xqf/2019-07-11T22-27-01-234Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 29/Step Study 29/Step_29 06-11/medial_chest/d5la7xya/2019-06-11T22-04-23-586Z\",\n",
    "          \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 30/Step Study 26/step 26/medial_chest/d5la7xya/2019-05-19T16-36-40-489Z\"]\n",
    "directs_thigh = [\"/mnt/storage/Datasets/Step Study/BioStampRC/Step 01/Step Study 01/step_01/rectus_femoris_right/d5la7xqf/2018-11-08T23-49-59-420Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 02/Step Study 02/rectus_femoris_left/d5la7xqf/2018-11-14T23-45-06-443Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 03/Step 03/step 03/rectus_femoris_left/d5la7xqf/2018-12-06T23-13-44-309Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 04/Step Study 04/step4/rectus_femoris_left/d5la7xqf/2019-01-25T23-06-51-946Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 05/Step Study 05/step05/rectus_femoris_right/d5la7xya/2019-02-06T16-04-27-147Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 06/Step Study 06/step 06/rectus_femoris_right/d5la7xya/2019-02-08T22-52-32-026Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 07/Step Study 07/step 07/rectus_femoris_left/d5la7xya/2019-02-23T18-41-11-381Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 08/Step Study 04/step4/rectus_femoris_left/d5la7xqf/2019-01-28T16-36-22-415Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 10/Step Study 10/step 10/rectus_femoris_left/d5la7xqf/2019-02-21T23-25-06-684Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 11/Step Study 11/step 11/rectus_femoris_right/d5la7xya/2019-02-23T00-54-20-107Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 12/Step Study 12/step 12/rectus_femoris_left/d5la7xqf/2019-02-15T23-00-20-071Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 13/Step Study 13/step 13/rectus_femoris_left/d5la7xqf/2019-03-22T17-56-58-571Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 14/Step Study 14/step 14/rectus_femoris_right/d5la7xqf/2019-04-25T23-16-51-869Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 15/rectus_femoris_right/d5la7xqf/2019-06-27T00-30-06-148Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 16/step 16/rectus_femoris_left/d5la7xqf/2019-06-28T19-07-20-526Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 17/Step Study 17/step 17/rectus_femoris_right/d5la7xya/2019-03-14T23-12-20-154Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 18/Step Study 18/Step18/rectus_femoris_right/d5la7xya/2019-04-23T22-20-01-882Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 19/Step Study 19/step 19/rectus_femoris_left/d5la7xqf/2019-04-30T23-15-00-849Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 20/Step Study 20/step 20/rectus_femoris_right/d5la7xqf/2019-06-12T21-20-16-341Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 21/Step Study 21/rectus_femoris_left/d5la7xya/2019-07-10T18-38-15-527Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 22/Step Study 22/step 22/rectus_femoris_right/d5la7xqf/2019-04-29T21-22-15-119Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 23/Step Study 23/step 23/rectus_femoris_left/d5la7xqf/2019-04-22T23-53-39-778Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 24/Step Study 24/step 24/rectus_femoris_left/d5la7xya/2019-06-14T19-39-17-260Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 25/Step Study 25/step 25/rectus_femoris_left/d5la7xqf/2019-05-14T23-56-21-307Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 25/Step Study 25/step 25/rectus_femoris_left/d5la7xya/2019-05-14T23-53-09-801Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 27/Step Study 27/step 27/rectus_femoris_right/d5la7xqf/2019-05-03T22-16-07-385Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 28/Step Study 28/step study 28/rectus_femoris_left/d5la7xya/2019-07-11T22-27-04-903Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 29/Step Study 29/Step_29 06-11/rectus_femoris_right/d5la7xqf/2019-06-11T22-04-29-829Z\",\n",
    "                \"/mnt/storage/Datasets/Step Study/BioStampRC/Step 30/Step Study 26/step 26/rectus_femoris_right/d5la7xqf/2019-05-19T16-36-44-148Z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of funcitons in this the data aggregation part:\n",
    "\n",
    "`get_time` gets time, duh.\n",
    "\n",
    "`chest_thigh_extraction` does the extractions and feature engineering for raw data that has tmestamps. In this case, that was the chest and thigh sensors.\n",
    "\n",
    "`hip_wrist_extraction` does the extractions and feature engineering for raw data that is at 80 Hz with no timestamps. In this case, that was the hip and wrist sensors.\n",
    "\n",
    "`hip_wrist_timestep_file` gets timestamps for the hip and wrist raw data.\n",
    "\n",
    "`add_hrs_to_time` adds hrs to a timestamp.\n",
    "\n",
    "`add_secs_to_time` adds secs to a timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(timestamp):\n",
    "    return timestamp.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data aggregation for the chest and thigh data.\n",
    "\n",
    "def chest_thigh_extraction(lis):\n",
    "    full_data = pd.DataFrame()\n",
    "    for i in lis:\n",
    "        chest = pd.read_csv(i + \"/accel.csv\")\n",
    "        chest['from_zero'] = (chest['Timestamp (ms)'] - chest.loc[0]['Timestamp (ms)']) // 1000\n",
    "        #chest['Timestamp (real.time)'] = chest['Timestamp (ms)'].apply(lambda x: datetime.datetime.fromtimestamp(x / 1000).strftime('%m/%d/%Y %H:%M:%S'))\n",
    "        chest['vm'] = ((chest['Accel X (g)']**2) + chest['Accel Y (g)']**2 + chest['Accel Z (g)']**2)**0.5\n",
    "        means = chest.groupby('from_zero').mean()[['Accel X (g)','Accel Y (g)', 'Accel Z (g)','vm']]\n",
    "        x_ffts = pd.DataFrame.from_dict(dict(chest.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['Accel X (g)'], 15)))), orient='index', columns=['xfft' + str(x) for x in range(1, 16)])\n",
    "        y_ffts = pd.DataFrame.from_dict(dict(chest.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['Accel Y (g)'], 15)))), orient='index', columns=['yfft' + str(x) for x in range(1, 16)])\n",
    "        z_ffts = pd.DataFrame.from_dict(dict(chest.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['Accel Z (g)'], 15)))), orient='index', columns=['zfft' + str(x) for x in range(1, 16)])\n",
    "        m_ffts = pd.DataFrame.from_dict(dict(chest.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['vm'], 15)))), orient='index', columns=['mfft' + str(x) for x in range(1, 16)])\n",
    "        xy_ffts = x_ffts.merge(y_ffts,left_index=True,right_index=True)\n",
    "        zm_ffts = z_ffts.merge(m_ffts,left_index=True,right_index=True)\n",
    "        ffts = xy_ffts.merge(zm_ffts,left_index=True,right_index=True)\n",
    "        means = means.merge(ffts,left_index=True,right_index=True)\n",
    "        means.rename(columns = {\"Accel X (g)\":\"mean.x\",\n",
    "                        \"Accel Y (g)\":\"mean.y\",\n",
    "                        \"Accel Z (g)\":\"mean.z\",\n",
    "                        \"vm\":\"mean.vm\"},inplace=True)\n",
    "        sds = chest.groupby('from_zero').std()[['Accel X (g)','Accel Y (g)', 'Accel Z (g)','vm']]\n",
    "        sds.rename(columns = {\"Accel X (g)\":\"sd.x\",\n",
    "                      \"Accel Y (g)\":\"sd.y\",\n",
    "                      \"Accel Z (g)\":\"sd.z\",\n",
    "                      \"vm\":\"sd.vm\"},inplace=True)\n",
    "        mins = chest.groupby('from_zero').min()[['Timestamp (ms)','Accel X (g)','Accel Y (g)', 'Accel Z (g)']]\n",
    "        mins.rename(columns = {\"Timestamp (ms)\":\"actual.time\",\n",
    "                       \"Accel X (g)\":\"min.x\",\n",
    "                       \"Accel Y (g)\":\"min.y\",\n",
    "                       \"Accel Z (g)\":\"min.z\"},inplace=True)\n",
    "        maxs = chest.groupby('from_zero').max()[['Accel X (g)','Accel Y (g)', 'Accel Z (g)']]\n",
    "        maxs.rename(columns = {\"Accel X (g)\":\"max.x\",\n",
    "                       \"Accel Y (g)\":\"max.y\",\n",
    "                       \"Accel Z (g)\":\"max.z\"},inplace=True)\n",
    "\n",
    "        quarter = chest.groupby('from_zero').quantile(0.25)[['vm']].rename(columns = {\"vm\":\"pct25\"})\n",
    "        three_quarter = chest.groupby('from_zero').quantile(0.75)[['vm']].rename(columns = {\"vm\":\"pct75\"})\n",
    "        max_min = pd.merge(mins,maxs,left_index=True,right_index=True)\n",
    "        mean_sd = pd.merge(means,sds,left_index=True,right_index=True)\n",
    "        quarts = pd.merge(quarter,three_quarter,left_index=True,right_index=True)\n",
    "        one_more_step = pd.merge(max_min,mean_sd,left_index=True,right_index=True)\n",
    "        raw_stats = pd.merge(one_more_step,quarts,left_index=True,right_index=True)\n",
    "        raw_stats['mean.xy'] = raw_stats['mean.x']*raw_stats['mean.y']\n",
    "        raw_stats['mean.xz'] = raw_stats['mean.x']*raw_stats['mean.z']\n",
    "        raw_stats['mean.yz'] = raw_stats['mean.y']*raw_stats['mean.z']\n",
    "        raw_stats['mean.xyz'] = raw_stats['mean.x']*raw_stats['mean.y']*raw_stats['mean.z']\n",
    "        raw_stats['mean.ang'] = 90*np.arcsin(raw_stats['mean.x']/raw_stats['mean.vm'])*(np.pi/2)\n",
    "        raw_stats['study'] = i.split(\"/\")[6]\n",
    "        raw_stats = raw_stats.reset_index()\n",
    "        full_data = full_data.append(raw_stats,ignore_index=True)\n",
    "    full_data['Timestamp (real.time)'] = full_data['actual.time'].apply(lambda x: datetime.datetime.fromtimestamp(x / 1000).strftime('%m/%d/%Y %H:%M:%S'))\n",
    "    full_data['Timestamp (real.time)'] = full_data['Timestamp (real.time)'].apply(datetime.datetime.strptime,args=('%m/%d/%Y %H:%M:%S',))\n",
    "    full_data['time'] = full_data['Timestamp (real.time)'].apply(get_time)\n",
    "    full_data['time'] = full_data['Timestamp (real.time)'].apply(add_hrs_to_time,args=(-8,))\n",
    "    full_data = full_data.drop(columns=['from_zero','actual.time'])\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hip_wrist_extraction(lis,type_of_sensor):\n",
    "    full_data = pd.DataFrame()\n",
    "    hip_directs = os.listdir(\"/mnt/storage/Datasets/Step Study/AG_hip_1sec\")\n",
    "    wrist_directs = os.listdir(\"/mnt/storage/Datasets/Step Study/wrist_CSV_1sec\")\n",
    "    for i in lis:\n",
    "        \n",
    "        hip = pd.read_csv(i,skiprows=10)\n",
    "        ht = hip.reset_index()\n",
    "        ht = ht.rename(columns={\"index\":\"sec\"})\n",
    "        ht['from_zero'] = ht['sec'] // 80\n",
    "        ht['vm'] = ((ht['Accelerometer X']**2)\n",
    "                       + ht['Accelerometer Y']**2\n",
    "                       + ht['Accelerometer Z']**2)**0.5\n",
    "        \n",
    "        means = ht.groupby('from_zero').mean()[['Accelerometer X',\n",
    "                                                   'Accelerometer Y',\n",
    "                                                   'Accelerometer Z',\n",
    "                                                   'vm']]\n",
    "        \n",
    "        x_ffts = pd.DataFrame.from_dict(dict(ht.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['Accelerometer X'], 15)))), orient='index', columns=['xfft' + str(x) for x in range(1, 16)])\n",
    "        y_ffts = pd.DataFrame.from_dict(dict(ht.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['Accelerometer Y'], 15)))), orient='index', columns=['yfft' + str(x) for x in range(1, 16)])\n",
    "        z_ffts = pd.DataFrame.from_dict(dict(ht.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['Accelerometer Z'], 15)))), orient='index', columns=['zfft' + str(x) for x in range(1, 16)])\n",
    "        m_ffts = pd.DataFrame.from_dict(dict(ht.groupby('from_zero').apply(lambda x: np.real(np.fft.fft(x['vm'], 15)))), orient='index', columns=['mfft' + str(x) for x in range(1, 16)])\n",
    "        xy_ffts = x_ffts.merge(y_ffts,left_index=True,right_index=True)\n",
    "        zm_ffts = z_ffts.merge(m_ffts,left_index=True,right_index=True)\n",
    "        ffts = xy_ffts.merge(zm_ffts,left_index=True,right_index=True)\n",
    "        \n",
    "        means = means.merge(ffts,left_index=True,right_index=True)\n",
    "        means.rename(columns = {\"Accelerometer X\":\"mean.x\",\n",
    "                        \"Accelerometer Y\":\"mean.y\",\n",
    "                        \"Accelerometer Z\":\"mean.z\",\n",
    "                        \"vm\":\"mean.vm\"},inplace=True)\n",
    "        sds = ht.groupby('from_zero').std()[['Accelerometer X',\n",
    "                                                'Accelerometer Y',\n",
    "                                                'Accelerometer Z',\n",
    "                                                'vm']]\n",
    "        sds.rename(columns = {\"Accelerometer X\":\"sd.x\",\n",
    "                      \"Accelerometer Y\":\"sd.y\",\n",
    "                      \"Accelerometer Z\":\"sd.z\",\n",
    "                      \"vm\":\"sd.vm\"},inplace=True)\n",
    "        mins = ht.groupby('from_zero').min()[['sec',\n",
    "                                                 'Accelerometer X',\n",
    "                                                 'Accelerometer Y',\n",
    "                                                 'Accelerometer Z']]\n",
    "        mins.rename(columns = {\"sec\":\"actual.time\",\n",
    "                       \"Accelerometer X\":\"min.x\",\n",
    "                       \"Accelerometer Y\":\"min.y\",\n",
    "                       \"Accelerometer Z\":\"min.z\"},inplace=True)\n",
    "        maxs = ht.groupby('from_zero').max()[['Accelerometer X',\n",
    "                                                 'Accelerometer Y',\n",
    "                                                 'Accelerometer Z']]\n",
    "        maxs.rename(columns = {\"Accelerometer X\":\"max.x\",\n",
    "                       \"Accelerometer Y\":\"max.y\",\n",
    "                       \"Accelerometer Z\":\"max.z\"},inplace=True)\n",
    "\n",
    "        quarter = ht.groupby('from_zero').quantile(0.25)[['vm']].rename(columns = {\"vm\":\"pct25\"})\n",
    "        three_quarter = ht.groupby('from_zero').quantile(0.75)[['vm']].rename(columns = {\"vm\":\"pct75\"})\n",
    "        max_min = pd.merge(mins,maxs,left_index=True,right_index=True)\n",
    "        mean_sd = pd.merge(means,sds,left_index=True,right_index=True)\n",
    "        quarts = pd.merge(quarter,three_quarter,left_index=True,right_index=True)\n",
    "        one_more_step = pd.merge(max_min,mean_sd,left_index=True,right_index=True)\n",
    "        raw_stats = pd.merge(one_more_step,quarts,left_index=True,right_index=True)\n",
    "        \n",
    "        raw_stats['mean.xy'] = raw_stats['mean.x']*raw_stats['mean.y']\n",
    "        raw_stats['mean.xz'] = raw_stats['mean.x']*raw_stats['mean.z']\n",
    "        raw_stats['mean.yz'] = raw_stats['mean.y']*raw_stats['mean.z']\n",
    "        raw_stats['mean.xyz'] = raw_stats['mean.x']*raw_stats['mean.y']*raw_stats['mean.z']\n",
    "        raw_stats['mean.ang'] = 90*np.arcsin(raw_stats['mean.x']/raw_stats['mean.vm'])*(np.pi/2)\n",
    "        \n",
    "        num = i.split(\"/\")[5].split(\"_\")[1]\n",
    "        raw_stats['study'] = \"Step \" + num\n",
    "        step_one = \"_\" + num\n",
    "        \n",
    "        if type_of_sensor == 'wrist':\n",
    "            timestep_file = [i for i in wrist_directs if step_one in i][0]\n",
    "        else:\n",
    "            timestep_file = [i for i in hip_directs if step_one in i][0]\n",
    "            \n",
    "        raw_stats = hip_wrist_timestep_file(timestep_file,raw_stats,type_of_sensor)\n",
    "        raw_stats['timestamp'] = raw_stats['timestamp'].apply(datetime.datetime.strptime,args=('%Y-%m-%d %H:%M:%S',))\n",
    "        raw_stats['time'] = raw_stats['timestamp'].apply(get_time)\n",
    "        \n",
    "        raw_stats = raw_stats.reset_index()\n",
    "        full_data = full_data.append(raw_stats,ignore_index=True)\n",
    "        full_data = full_data.drop(columns=['from_zero','actual.time'])\n",
    "        \n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hip_wrist_timestep_file(file,raw_stats,type_of_sensor):\n",
    "    if type_of_sensor == 'hip':\n",
    "        timestamps = pd.read_csv(\"/mnt/storage/Datasets/Step Study/AG_hip_1sec/\" + file,skiprows=10)['timestamp']\n",
    "    if type_of_sensor == 'wrist':\n",
    "        timestamps = pd.read_csv(\"/mnt/storage/Datasets/Step Study/wrist_CSV_1sec/\" + file,skiprows=10)['timestamp']\n",
    "    \n",
    "    return raw_stats.join(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hrs_to_time(timeval, to_add):\n",
    "    \n",
    "    if timeval.hour + to_add < 0:\n",
    "        \n",
    "        hours = 24 + timeval.hour + to_add\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        hours = timeval.hour + to_add\n",
    "    \n",
    "    return dt.time(hours,timeval.minute,timeval.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_secs_to_time(timeval, to_add):\n",
    "\n",
    "    secs = timeval.hour * 3600 + timeval.minute * 60 + timeval.second\n",
    "\n",
    "    secs += to_add\n",
    "\n",
    "    return dt.time(secs // 3600, (secs % 3600) // 60, secs % 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chest = chest_thigh_extraction(directs_chest)\n",
    "thigh = chest_thigh_extraction(directs_thigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip = hip_wrist_extraction(glob.glob(\"/home/skeadle/step study/hip_csv/*.csv\"),'hip')\n",
    "wrist = hip_wrist_extraction(glob.glob(\"/home/skeadle/step study/wrist_csv/*.csv\"),'wrist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOG AGGREGATiION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = pd.read_excel(\"/mnt/storage/Datasets/Step Study/Complete Data.xlsx\")\n",
    "complete_data = complete_data[complete_data['activity'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a list of functions in the log aggregation function:\n",
    "\n",
    "`get_simple_encoding_type` gets the simple and detailed encoding type.\n",
    "\n",
    "`length_of_exercise` gets the length of the activity.\n",
    "\n",
    "`extending_log` does some reshaping to help with the merge later on.\n",
    "\n",
    "`combine_log_and_data` combines the raw data and the log to get the complete data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_encoding_type(data):\n",
    "    broad = []\n",
    "    detailed = []\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            activity = row['activity'].replace(\"(\",\":\").split(\":\")\n",
    "            if 'Self' in activity[1]:\n",
    "                broad.append(\"Walk\")\n",
    "                detailed.append(\"Self Paced NA\")\n",
    "            elif 'Driv' in activity[0]:\n",
    "                broad.append(\"Drive\")\n",
    "                if 'Fidgit' in activity[1]:\n",
    "                    activity[1] = \"Fidget Sit\"\n",
    "                detailed.append(\"Drive\" + \" \" + activity[1].strip())\n",
    "            else:\n",
    "                broad.append(\"Walk\")\n",
    "                detailed.append(activity[0].strip() + \" \" + activity[1].replace(\",\",\"\").strip())\n",
    "        except IndexError:\n",
    "            if row.activity.strip() == 'Driving':\n",
    "                broad.append(\"Drive\")\n",
    "                detailed.append(\"not applicable\")\n",
    "            else:\n",
    "                broad.append(row.activity.strip())\n",
    "                detailed.append(\"not applicable\")\n",
    "    data['broad_encoding'] = broad\n",
    "    data['detailed_encoding'] = detailed\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_of_exercise(data):\n",
    "    exercise_time_label = []\n",
    "    for index,row in data.iterrows():\n",
    "        if row.broad_encoding == 'Walk':\n",
    "            if row.time_sec/60 < 1.0:\n",
    "                exercise_time_label.append(\"<1 min\")\n",
    "            elif (row.time_sec/60 >= 1.0) and (row.time_sec/60 < 5.0):\n",
    "                exercise_time_label.append(\"<5 min and >1 min\")\n",
    "            elif (row.time_sec/60 >= 5.0) and (row.time_sec/60 < 10.0):\n",
    "                exercise_time_label.append(\"<10 min and >5 min\")\n",
    "            else:\n",
    "                exercise_time_label.append(\">10 min\")\n",
    "        else:\n",
    "            exercise_time_label.append(\"not applicable\")\n",
    "    data['duration_label'] = exercise_time_label\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extending_log(dat):\n",
    "    log = pd.DataFrame()\n",
    "    dat = dat[dat['AG_wrist_time'].notna()]\n",
    "    for index,row in dat.iterrows():\n",
    "        start = row.start_time\n",
    "        stop = row.stop_time\n",
    "        while start <= stop:\n",
    "            record = []\n",
    "            record.append(start)\n",
    "            record.append(row.activity)\n",
    "            record.append(row.broad_encoding)\n",
    "            record.append(row.detailed_encoding)\n",
    "            record.append(row.duration_label)\n",
    "            log = log.append(pd.Series(record),ignore_index = True)\n",
    "            start = add_secs_to_time(start,1)\n",
    "    log.rename(columns = {0:'time',1:'activity',2:'broad_encoding',3:'detailed_encoding',4:'duration'},inplace=True)\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERGING SENSOR AND LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_log_and_data(sensor_data,log_data):\n",
    "    final_data = pd.DataFrame()\n",
    "    for i in list(range(1,len(log_data.ID.unique()) + 1)):\n",
    "        if i < 10:\n",
    "            identifier_log = \"SS0\" + str(i)\n",
    "            identifier_study = \"Step 0\" + str(i)\n",
    "        else:\n",
    "            identifier_log = \"SS\" + str(i)\n",
    "            identifier_study = \"Step \" + str(i)\n",
    "        log = extending_log(log_data[log_data.ID == identifier_log])\n",
    "        sensor = sensor_data.loc[sensor_data.study == identifier_study].reset_index()\n",
    "        merged = log.merge(sensor,on=\"time\")\n",
    "        final_data = final_data.append(merged,ignore_index=True)\n",
    "    final_data = final_data.drop(columns=['index'])\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_encoded = get_simple_encoding_type(complete_data)\n",
    "new_encoded = length_of_exercise(one_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_final = combine_log_and_data(chest,new_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "thigh_final = combine_log_and_data(thigh,new_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_final = combine_log_and_data(wrist,new_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip_final = combine_log_and_data(hip,new_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_final.to_csv(\"/home/auschwar/step_chest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "thigh_final.to_csv(\"/home/auschwar/step_thigh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_final.to_csv(\"/home/auschwar/step_wrist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip_final.to_csv(\"/home/auschwar/step_hip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
