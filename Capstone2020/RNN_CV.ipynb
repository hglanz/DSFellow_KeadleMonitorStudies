{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `segments` dictionary translates the primary behaviors to the overall segemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for neural network training\n",
    "NUM_EPOCHS = 30\n",
    "# Sensors for training\n",
    "sensors = ['Hip', 'Wrist', 'Chest', 'Thigh']\n",
    "segments = {\n",
    "    \"LES- socializing, communicating, leisure time not screen\":\"LES\",\n",
    "      \"LES- screen based leisure time (TV, video game, computer)\":\"LES\",\n",
    "      \"HA- animals and pets\":\"HA\",\n",
    "      \"HA- housework\":\"HA\",\n",
    "      \"HA- food prep and cleanup\":\"HA\",\n",
    "      \"HA- household management/other household activities\":\"HA\",\n",
    "      \"HA- interior maintenance, repair, & decoration\":\"HA\",\n",
    "      \"HA- lawn, garden and houseplants\":\"HA\",\n",
    "      \"HA- exterior maintenance, repair, & decoration\":\"HA\",\n",
    "      \"CA- caring for and helping adults\":\"HA\",\n",
    "      \"CA- caring for and helping children\":\"HA\",\n",
    "      \"WRK- screen based - Education and Health Services\":\"WRK\",\n",
    "      \"WRK- screen based - Office (business, professional services, finance, info)\":\"WRK\",\n",
    "      \"WRK- general - Education and Health Services\":\"WRK\",\n",
    "      \"WRK- general - Office (business, professional services, finance, info)\":\"WRK\",\n",
    "      \"EDU- taking class, research, homework\":\"WRK\",\n",
    "      \"PC- groom, health-related\":\"HA\",\n",
    "      \"PC- other personal care\":\"HA\",\n",
    "      \"EAT- eating and drinking, waiting\":\"HA\",\n",
    "      \"PUR- purchasing goods and services\":\"HA\",\n",
    "      \"ORG- organizational civic, volunteer, and religious activities\":\"COM\",\n",
    "      \"EX- attending sport, recreational event, or performance\":\"COM\",\n",
    "      \"EX- walking\":\"EX\",\n",
    "      \"EX- jogging\":\"EX\",\n",
    "      \"EX- baseball\":\"EX\",\n",
    "      \"EX- other\":\"EX\",\n",
    "      \"TRAV- walking\":\"COM\",\n",
    "      \"TRAV- passenger bus or train\":\"COM\",\n",
    "      \"TRAV- biking\":\"COM\",\n",
    "      \"TRAV- driver (car/truck/motorcycle)\":\"COM\",\n",
    "      \"OTHER- non codable\":\"OTHER\",\n",
    "      \"None\":\"OTHER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from running the data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single sensor neural network for detailed activity labels\n",
    "activity = 'updated_final_activity'\n",
    "losses = []\n",
    "accs = []\n",
    "preds = []\n",
    "s = \"Hip\"\n",
    "\n",
    "# attain dataset for running the model \n",
    "data = clean_data(s, activity, True)\n",
    "\n",
    "# run the model and output revelant results \n",
    "acc, cnf_mat = run_nn(data, \"Hip\", activity, post = False)\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Accuracy: \",acc)\n",
    "print(cnf_mat)\n",
    "print_confusion_matrix(cnf_mat, ['sit/lie', 'stand and move', 'walking', 'running','bicycling'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single sensor neural network for detailed activity labels\n",
    "activity = 'updated_final_activity'\n",
    "losses = []\n",
    "accs = []\n",
    "preds = []\n",
    "s = \"Chest\"\n",
    "data = clean_data(s, activity, True)\n",
    "#res = run_nn(data, s, activity)\n",
    "#sum_cnf_matrix = np.sum(res[2], axis = 0)\n",
    "\n",
    "acc, cnf_mat = run_nn(data, \"Chest\", activity, post = False)\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Accuracy: \",acc)\n",
    "print_confusion_matrix(cnf_mat, ['sit/lie', 'stand and move', 'walking', 'running','bicycling'])\n",
    "\n",
    "#pred = pd.DataFrame(pred)\n",
    "\n",
    "#pred.to_csv(''+s+'_predictions.csv')\n",
    "\n",
    "#df = pd.DataFrame({'pred':preds})\n",
    "#sensor = \"Hip\"\n",
    "#df.to_csv(sensor+'_'+'predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single sensor neural network for detailed activity labels\n",
    "activity = 'updated_final_activity'\n",
    "losses = []\n",
    "accs = []\n",
    "preds = []\n",
    "s = \"Wrist\"\n",
    "data = clean_data(s, activity, True)\n",
    "#res = run_nn(data, s, activity)\n",
    "#sum_cnf_matrix = np.sum(res[2], axis = 0)\n",
    "acc, cnf_mat = run_nn(data, \"Wrist\", activity, post = False)\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Accuracy: \",acc)\n",
    "print_confusion_matrix(cnf_mat, ['sit/lie', 'stand and move', 'walking', 'running','bicycling'])\n",
    "\n",
    "#pred = pd.DataFrame(pred)\n",
    "\n",
    "#pred.to_csv(''+s+'_predictions.csv')\n",
    "\n",
    "#df = pd.DataFrame({'pred':preds})\n",
    "#sensor = \"Hip\"\n",
    "#df.to_csv(sensor+'_'+'predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thigh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single sensor neural network for detailed activity labels\n",
    "activity = 'updated_final_activity'\n",
    "losses = []\n",
    "accs = []\n",
    "preds = []\n",
    "s = \"Thigh\"\n",
    "data = clean_data(s, activity,True)\n",
    "#res = run_nn(data, s, activity)\n",
    "#sum_cnf_matrix = np.sum(res[2], axis = 0)\n",
    "\n",
    "acc, cnf_mat = run_nn(data, \"Thigh\", activity, post = False)\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Accuracy: \",acc)\n",
    "print_confusion_matrix(cnf_mat, ['sit/lie', 'stand and move', 'walking', 'running','bicycling'])\n",
    "\n",
    "#pred = pd.DataFrame(pred)\n",
    "\n",
    "#pred.to_csv(''+s+'_predictions.csv')\n",
    "\n",
    "#df = pd.DataFrame({'pred':preds})\n",
    "#sensor = \"Hip\"\n",
    "#df.to_csv(sensor+'_'+'predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix(cnf_mat, ['sit/lie', 'stand and move', 'walking', 'running','bicycling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1.43116e+05, 2.43170e+04, 1.38000e+02, 2.00000e+00, 1.14000e+02],\n",
    " [2.55780e+04, 9.64500e+04, 5.27300e+03, 2.29000e+02, 2.44000e+02],\n",
    " [2.70000e+01, 6.00600e+03, 2.42410e+04, 9.20000e+01, 1.00000e+01],\n",
    " [5.90000e+01, 4.20000e+01, 7.56000e+02, 8.32900e+03, 1.20000e+01],\n",
    " [1.49000e+02, 5.74000e+03, 2.18000e+02, 1.00000e+00, 0.00000e+00]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `clean_data` deals with importing the dataset and computing the segemnts\n",
    "- `run_nn` prepares some features and runs the neural network\n",
    "    - `run_kfold` is called within this function, it runs the stratified cross-validation and computes the averaged accuracy and other metrics\n",
    "- `get_model_1layer_updated` is an example of the place to put any varying modeling architecture, to change the model just make a new function and edit the line in `run_kfold` that goes `model = get_model_1layer_updated()` with your new model\n",
    "- `print_confusion_matrix` displays a confusion matrix\n",
    "- `post_process` the prediction post processing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(sensor, activity, halved = False):\n",
    "    \"\"\"\n",
    "    Imports the data, adds and drops appropriate columns, and adds segementation to the dataset\n",
    "    \"\"\"\n",
    "    data = pd.read_csv('../../mnt/storage/Datasets/final_data/'+sensor + \".csv\", low_memory = False)\n",
    "    # Take out unnamed columns\n",
    "    if halved:\n",
    "        data_halved = data[data.updated_final_activity != 'sit/lie']\n",
    "        data2 = data[data.updated_final_activity == 'sit/lie'].sample(frac = 0.5)\n",
    "        data = data_halved.append(data2)\n",
    "    \n",
    "    data.drop([col for col in data.columns if \"Unnamed\" in col], axis=1, inplace = True)\n",
    "    \n",
    "    # Drop un-encoded data\n",
    "    data = data[data[activity] != 'private/not coded'].reset_index()\n",
    "    # add segment activity\n",
    "        \n",
    "    data[\"segment\"] = data[\"primary_behavior\"].map(segments)\n",
    "    print(data.shape)\n",
    "    \n",
    "    # enumerate segments\n",
    "    # any time the primary behavior changes, we are in a new segment\n",
    "    number = 0\n",
    "    segment_group = []\n",
    "    for index, row in data.iterrows():\n",
    "        #print(index)\n",
    "        if index == 0:\n",
    "            segment_group.append(number)\n",
    "            #segment_type.append(hip.loc[index,\"segment\"])\n",
    "        else:\n",
    "            current_segment = data.loc[index,\"segment\"]\n",
    "            last_segment = data.loc[index - 1,\"segment\"]\n",
    "            if last_segment != current_segment:\n",
    "                # we are in a new segment\n",
    "                number += 1\n",
    "            segment_group.append(number)\n",
    "    data[\"enum_segment\"] = segment_group\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(data, sensor, activity, post = False):\n",
    "    \"\"\"\n",
    "    data: cleaned dataset\n",
    "    sensor: which sensor we are analyzing (e.g. Hip)\n",
    "    activity: column name of where the ground truth is\n",
    "    post: whether to apply post-processing smoothing to the predicted values or not\n",
    "    \"\"\"\n",
    "    x_cols = data.columns[19:(82 + 20)]\n",
    "    #print(x_cols)\n",
    "    y = data[activity]\n",
    "    #print(y.unique())\n",
    "    # Standardize x columns\n",
    "    X = data[x_cols].apply(lambda x: (x - np.mean(x)) / np.std(x))\n",
    "    X = X.fillna(X.mean()).values\n",
    "    X = pd.DataFrame(X)\n",
    "    # add segementation and enumeration\n",
    "    X[\"segment\"] = data.loc[:,\"segment\"]\n",
    "    X[\"enum_segment\"] = data.loc[:,\"enum_segment\"]\n",
    "    \n",
    "    # One hot vectorize categories\n",
    "    Y = pd.get_dummies(y).values\n",
    "    dummy_labels = pd.get_dummies(y).columns.tolist()\n",
    "    \n",
    "    # Run k-fold cross validation\n",
    "    res = run_kfold(X, Y, 5, sensor, activity, dummy_labels, post_process, True)\n",
    "    \n",
    "    # Save confusion matrix visualizations\n",
    "    #conf_mat(res, sensor, activity, dummy_labels)\n",
    "    # Calculate precision/recall metrics, save results\n",
    "    #prec_recall(res, sensor, activity, dummy_labels)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(X, Y, K, sensor, activity, class_labels, post,rnn = True):\n",
    "    acc_sum = 0\n",
    "    loss_sum = 0\n",
    "    prec = []\n",
    "    cnf_tables = []\n",
    "    cnf_total = np.zeros((5,5))\n",
    "    f1_rep = []\n",
    "    prec_rep = []\n",
    "    rec_rep = []\n",
    "    i = 1\n",
    "    model = None\n",
    "    overall_categories = list(X[\"segment\"].unique())\n",
    "    for k in range(K):\n",
    "        # first, select test train split via stratified K-fold cross-validation\n",
    "        # that is, within each category, do K-fold\n",
    "        train = []\n",
    "        test = []\n",
    "        for cat in overall_categories:\n",
    "            # extract all rows with this segement and get their enumerated segment\n",
    "            subset = X[X[\"segment\"] == cat]\n",
    "            indexes = list(subset[\"enum_segment\"].unique())\n",
    "\n",
    "            # then stratified train test split\n",
    "            train += [x for i,x in enumerate(indexes) if i % K != k]\n",
    "            test += [x for i,x in enumerate(indexes) if i % K == k]\n",
    "        \n",
    "        # Reset model and get new one\n",
    "        model = get_model_1layer_updated()\n",
    "\n",
    "        # Split data into train and test\n",
    "        X_train, X_test = X[X[\"enum_segment\"].isin(train)], X[X[\"enum_segment\"].isin(test)]\n",
    "        y_train, y_test = Y[X_train.index], Y[X_test.index]\n",
    "            \n",
    "        # drop segment related columns\n",
    "        X_train.drop(columns = [\"enum_segment\",\"segment\"], inplace = True)\n",
    "        X_test.drop(columns = [\"enum_segment\",\"segment\"], inplace = True)\n",
    "        \n",
    "        if rnn:\n",
    "            X_train,y_train = aggregate(np.array(X_train),np.array(y_train),7)\n",
    "            X_test, y_test = aggregate(np.array(X_test),np.array(y_test),7)\n",
    "            model = get_rnn(X_train)\n",
    "        # fit the model\n",
    "        model.fit(X_train, y_train,batch_size=128, epochs= NUM_EPOCHS,\n",
    "                  verbose = False)\n",
    "        \n",
    "        # Confusion matrix calculations\n",
    "        \n",
    "        # Get model predictions, save as list\n",
    "        if rnn:\n",
    "            reshaped = y_test.shape[0]*y_test.shape[1]\n",
    "            pred = model.predict(X_test, verbose = 1)\n",
    "            pred = pred.reshape(reshaped,5)\n",
    "            y_pred = np.argmax(pred, axis=1)\n",
    "            y_pred = [class_labels[x] for x in y_pred.tolist()]\n",
    "        if not rnn:\n",
    "            pred = model.predict(X_test, verbose = 1)\n",
    "            y_pred = np.argmax(pred, axis=1)\n",
    "            y_pred = [class_labels[x] for x in y_pred.tolist()]\n",
    "        # post-process if necessary\n",
    "        if post:\n",
    "            y_pred = post_process(y_pred)\n",
    "        \n",
    "        if rnn:\n",
    "            reshaped = y_test.shape[0]*y_test.shape[1]\n",
    "            y_true = np.argmax(y_test.reshape(reshaped,5), axis = 1)\n",
    "            y_true = [class_labels[x] for x in y_true.tolist()]\n",
    "        # Get ground truth\n",
    "        if not rnn:\n",
    "            y_true = np.argmax(y_test, axis = 1)\n",
    "            y_true = [class_labels[x] for x in y_true.tolist()]\n",
    "        \n",
    "        print(y_pred[0:20])\n",
    "        print(y_true[0:20])\n",
    "        \n",
    "        # Evaluate accuracy of model\n",
    "        acc = sklearn.metrics.accuracy_score(y_true,y_pred)\n",
    "    \n",
    "        acc_sum = acc_sum + acc\n",
    "        i += 1\n",
    "\n",
    "        # Make confusion matrix\n",
    "        cnf_matrix = sklearn.metrics.confusion_matrix(y_true, y_pred,\n",
    "            labels=['sit/lie', 'stand and move', 'walking', 'running',\n",
    "               'bicycling'])\n",
    "\n",
    "        # Add confusion matrix for this fold to list\n",
    "        cnf_tables.append(cnf_matrix)\n",
    "        \n",
    "        # Add to total confusion matrix\n",
    "        cnf_total += cnf_matrix\n",
    "        \n",
    "        class_report = sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, \n",
    "            labels = ['sit/lie', 'stand and move', 'walking', 'running','bicycling'])\n",
    "        \n",
    "        # get f1 for this fold \n",
    "        f1_rep.append(class_report[2:4])\n",
    "        \n",
    "        # get precision for this fold\n",
    "        prec_rep.append(itemgetter(0,3)(class_report))\n",
    "        \n",
    "        # get recall for this fold\n",
    "        rec_rep.append(itemgetter(1,3)(class_report))\n",
    "        \n",
    "\n",
    "    # Save last trained model to disk\n",
    "    \n",
    "    # Get average loss and accuracy for all kfolds\n",
    "    acc_r = round(acc_sum / K, 3)\n",
    "    \n",
    "    # print out weighted average f1-score\n",
    "    actual_f1 = {'sit/lie':0, 'stand and move':0,'walking':0,'running':0,'bicycling':0}\n",
    "    total_f1 = {'sit/lie':0,'stand and move':0, 'walking':0,'running':0,'bicycling':0}\n",
    "    \n",
    "\n",
    "    for f1 in f1_rep:\n",
    "        actual_f1['sit/lie'] += f1[0][0] * f1[1][0]\n",
    "        actual_f1['stand and move'] += f1[0][1] * f1[1][1]\n",
    "        actual_f1[\"walking\"] += f1[0][2] * f1[1][2]\n",
    "        actual_f1[\"running\"] += f1[0][3] * f1[1][3]\n",
    "        actual_f1[\"bicycling\"] += f1[0][4] * f1[1][4]\n",
    "        total_f1['sit/lie'] += f1[1][0]\n",
    "        total_f1['stand and move'] += f1[1][1]\n",
    "        total_f1[\"walking\"] += f1[1][2]\n",
    "        total_f1[\"running\"] += f1[1][3]\n",
    "        total_f1[\"bicycling\"] += f1[1][4]\n",
    "        \n",
    "    for k in list(actual_f1.keys()):\n",
    "        print(\"F1 \",k,\":\",actual_f1[k]/total_f1[k])\n",
    "    \n",
    "    # print out precision stats\n",
    "    actual_prec = {'sit/lie':0, 'stand and move':0,'walking':0,'running':0,'bicycling':0}\n",
    "    total_prec = {'sit/lie':0, 'stand and move':0,'walking':0,'running':0,'bicycling':0}\n",
    "    \n",
    "    for prec in prec_rep:\n",
    "        actual_prec['sit/lie'] += prec[0][0] * prec[1][0]\n",
    "        actual_prec['stand and move'] += prec[0][1] * prec[1][1]\n",
    "        actual_prec[\"walking\"] += prec[0][2] * prec[1][2]\n",
    "        actual_prec[\"running\"] += prec[0][3] * prec[1][3]\n",
    "        actual_prec[\"bicycling\"] += prec[0][4] * prec[1][4]\n",
    "        total_prec['sit/lie'] += prec[1][0]\n",
    "        total_prec['stand and move'] += prec[1][1]\n",
    "        total_prec[\"walking\"] += prec[1][2]\n",
    "        total_prec[\"running\"] += prec[1][3]\n",
    "        total_prec[\"bicycling\"] += prec[1][4]\n",
    "        \n",
    "    for k in list(actual_prec.keys()):\n",
    "        print(\"Precision \",k,\":\",actual_prec[k]/total_prec[k])\n",
    "    \n",
    "    # print out recall stats\n",
    "    actual_re = {'sit/lie':0, 'stand and move':0,'walking':0,'running':0,'bicycling':0}\n",
    "    total_re = {'sit/lie':0, 'stand and move':0,'walking':0,'running':0,'bicycling':0}\n",
    "    \n",
    "    for rec in rec_rep:\n",
    "        actual_re['sit/lie'] += rec[0][0] * rec[1][0]\n",
    "        actual_re['stand and move'] += rec[0][1] * rec[1][1]\n",
    "        actual_re[\"walking\"] += rec[0][2] * rec[1][2]\n",
    "        actual_re[\"running\"] += rec[0][3] * rec[1][3]\n",
    "        actual_re[\"bicycling\"] += rec[0][4] * rec[1][4]\n",
    "        total_re['sit/lie'] += rec[1][0]\n",
    "        total_re['stand and move'] += rec[1][1]\n",
    "        total_re[\"walking\"] += rec[1][2]\n",
    "        total_re[\"running\"] += rec[1][3]\n",
    "        total_re[\"bicycling\"] += rec[1][4]\n",
    "        \n",
    "    for k in list(actual_re.keys()):\n",
    "        print(\"Recall \",k,\":\",actual_re[k]/total_re[k])\n",
    "    \n",
    "    return acc_r, cnf_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(X,Y,step = 5):\n",
    "    agged = []\n",
    "    agged_y = []\n",
    "    add = (len(X) - len(X) % step) \n",
    "    #print(add)\n",
    "    \n",
    "    X = X[:add,]\n",
    "    \n",
    "    Y = Y[:add,]\n",
    "    \n",
    "    #print(X[0:135,])\n",
    "    i=0\n",
    "    while i < len(X):\n",
    "        #print(i)\n",
    "        x = X[i:i+step,]\n",
    "        y = Y[i:i+step,]\n",
    "        #print(x.shape)\n",
    "        i+= step\n",
    "        if x.shape != (step,83):\n",
    "            print('yes',x)\n",
    "        agged.append(x)\n",
    "        agged_y.append(y)\n",
    "    agged = np.array(agged)\n",
    "    agged_y = np.array(agged_y)\n",
    "    \n",
    "    return agged, agged_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn(X_train):\n",
    "    '''builds rnn and returns model'''\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128,input_shape = (X_train.shape[1],X_train.shape[2]), return_sequences = True))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32, return_sequences = True))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    #model.add(Dropout(0.4))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_1layer_updated():\n",
    "    # Neural Network Architecture for single sensor\n",
    "    model = tf.keras.Sequential([\n",
    "            layers.Dense(83, activation=\"relu\", input_shape = (83,)),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            layers.Dense(16, activation=\"relu\"),\n",
    "            layers.Dense(8, activation=\"relu\"),\n",
    "            layers.Dense(5, activation = 'softmax')])\n",
    "    model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    #fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(preds):   \n",
    "    \"\"\"\n",
    "    basic post processing...\n",
    "    prediction: array of predicted values from the NN\n",
    "    \"\"\"\n",
    "    #preds = prediction['predicted_values']\n",
    "    preds_new = preds.copy()\n",
    "\n",
    "\n",
    "    for i in range(2, len(preds)-2):\n",
    "#         size = mode_size*2 +1\n",
    "#         points = [None]* size\n",
    "#         for j in range(size):\n",
    "#             print(j)\n",
    "#             print(points)\n",
    "#             points[j] = preds.iloc[i - mode_size]\n",
    "#             mode_size +=1\n",
    "        points = [preds[i-2],preds[i-1], preds[i], preds[i+1], preds[i+2]]\n",
    "        preds_new[i] = stats.mode(points)[0][0]\n",
    "\n",
    "    #preds_new = pd.DataFrame(preds_new)\n",
    "    #preds_new.columns = ['post_predictions']\n",
    "    #final_preds = pd.concat([prediction,preds_new],axis =1)\n",
    "\n",
    "    #errors_1 = final_preds['y_true'] == final_preds['predicted_values']\n",
    "    #errors_2 = final_preds['y_true'] == final_preds['post_predictions']\n",
    "\n",
    "    #print('predicted_value errors',np.sum(errors_1)/len(errors_1))\n",
    "\n",
    "    #print('post_processed errors',np.sum(errors_2)/len(errors_2))\n",
    "    \n",
    "    #print('Accuracy improvement:', (np.sum(errors_2)/len(errors_2)-np.sum(errors_1)/len(errors_1)))\n",
    "    return preds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Histograms\n",
    "sensors = [\"Hip\",\"Chest\",\"Wrist\", \"Thigh\"]\n",
    "#data = pd.read_csv('../../mnt/storage/Datasets/final_data/'+\"Hip\" + \".csv\", low_memory = False)\n",
    "#print(data.updated_final_activity.unique())\n",
    "\n",
    "\n",
    "data = clean_data(\"Hip\",'updated_final_activity',True)\n",
    "sit = len(data[data.updated_final_activity == 'sit/lie'])\n",
    "stand = len(data[data.updated_final_activity == 'stand and move']) \n",
    "walk = len(data[data.updated_final_activity == 'walking']) \n",
    "run = len(data[data.updated_final_activity == 'running'])\n",
    "bike = len(data[data.updated_final_activity == 'bicycling'])\n",
    "print([sit,stand,walk,run,bike])\n",
    "x = np.arange(5)\n",
    "plt.bar(x,height = [sit,stand,walk,run,bike])\n",
    "plt.xticks(x, ['sit/lie', 'stand and move', 'walking', 'running',\n",
    "'bicycling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.updated_final_activity != 'sit/lie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
